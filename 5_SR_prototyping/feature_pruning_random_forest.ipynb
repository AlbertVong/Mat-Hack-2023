{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "par = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "data_path = os.path.join(par,'3_generate_features','dimensionless_cropped_final_feature_array.csv')\n",
    "label_path = os.path.join(par,'3_generate_features','final_label_array.csv')\n",
    "\n",
    "os.path.isfile(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to change multiclass classification to 1 vs all\n",
    "def multiclass_to_binary(labels, most_common_id):\n",
    "    to_binary = lambda val: 1 if val == most_common_id else 0\n",
    "    to_binary_vec = np.vectorize(to_binary)\n",
    "\n",
    "    labels_1vsall = to_binary_vec(labels)\n",
    "\n",
    "    return labels_1vsall\n",
    "\n",
    "\n",
    "def subsample_data(features,labels_1vsall,n_subsample):\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    #Dominant class boolean index\n",
    "    positive_class_mask = labels_1vsall==1\n",
    "    \n",
    "    #Dominant class indexing to grab for training/test set (we want 50/50 representation)\n",
    "    SR_features_train = features[positive_class_mask,:]\n",
    "    SR_target_train = labels_1vsall[positive_class_mask]\n",
    "\n",
    "    #Grabbing all negative examples of which we're going to grab a number equal to the number of dominant class\n",
    "    SR_negative_train = features[~positive_class_mask,:]\n",
    "    SR_negative_target = labels_1vsall[~positive_class_mask]\n",
    "\n",
    "    #Apply subsampling. We grab a random subset from the negative set of the same size as the positive examples\n",
    "    subsample_idx = rng.permutation(SR_negative_target.size)[:n_subsample]\n",
    "\n",
    "    #Concatenate an equal amount of negative training data to the list of positive training data so we have 50/50 class representation\n",
    "    SR_features = np.concatenate((SR_features_train, SR_negative_train[subsample_idx,:]),axis=0)\n",
    "    SR_targets = np.concatenate((SR_target_train, SR_negative_target[subsample_idx]), axis=0)\n",
    "\n",
    "    return SR_features, SR_targets\n",
    "\n",
    "\n",
    "def run_random_forest(features,labels_1vsall):\n",
    "    #Stratified K fold (maintain class balance)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    cv_precisions = []\n",
    "    cv_recalls = []\n",
    "\n",
    "    aggregated_feature_importances = []\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(features,labels_1vsall)):\n",
    "        train_features_i = features[train_idx]\n",
    "        train_labels_i = labels_1vsall[train_idx]\n",
    "\n",
    "        test_features_i = features[test_idx]\n",
    "        test_labels_i = labels_1vsall[test_idx]\n",
    "\n",
    "        #Fit naive rf model\n",
    "        naive_rf_i = RandomForestClassifier()\n",
    "        naive_rf_i.fit(train_features_i, train_labels_i)\n",
    "        \n",
    "        predict_labels_i = naive_rf_i.predict(test_features_i)\n",
    "\n",
    "        #Fit on k-folded validation set\n",
    "        precision, recall = precision_score(test_labels_i, predict_labels_i), recall_score(test_labels_i, predict_labels_i)\n",
    "\n",
    "        aggregated_feature_importances += [np.array(naive_rf_i.feature_importances_)]\n",
    "        \n",
    "        cv_precisions += [precision]\n",
    "        cv_recalls += [recall]\n",
    "\n",
    "    aggregated_feature_importances = np.array(aggregated_feature_importances)\n",
    "    aggregated_feature_importances = np.mean(aggregated_feature_importances, axis=0)\n",
    "    sorted_indices = np.argsort(aggregated_feature_importances)\n",
    "    sorted_importances = aggregated_feature_importances[sorted_indices]\n",
    "    sorted_feature_names = np.array(feature_names)[sorted_indices]\n",
    "    feature_info = pd.DataFrame(data={'Name':list(sorted_feature_names),\n",
    "                                      \n",
    "    'Importances':sorted_importances}).sort_values(by='Importances',ascending=False,inplace=False)\n",
    "    feature_info.to_csv(f'features/{feature_list_number}_iteration_feature_{len(feature_names)}_features.csv')\n",
    "    return np.mean(cv_precisions), np.mean(cv_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeats, precs, recalls = [], [], []\n",
    "\n",
    "feature_names = np.zeros(999999)\n",
    "\n",
    "while len(feature_names) > 1:\n",
    "    print(f'{len(feature_names)} features remaining')\n",
    "    #Turn into DF\n",
    "    feat_df = pd.read_csv(data_path, index_col=0)\n",
    "    label_df = pd.read_csv(label_path)\n",
    "    \n",
    "    #Dataframe convert label into categorical variable for classification\n",
    "    #Then convert labels into numpy array\n",
    "    label_name = 'Prototype'\n",
    "    label_df[label_name]= pd.Categorical(label_df[label_name])\n",
    "    label_df['numeric_label'] = label_df[label_name].cat.codes\n",
    "\n",
    "    #Convert numerical dataframe column to array\n",
    "    labels = label_df['numeric_label'].to_numpy()\n",
    "\n",
    "    #Convert features to numpy\n",
    "    #Also define feature names for symbolic regression\n",
    "    features = feat_df.to_numpy()\n",
    "    feature_names = np.array(feat_df.columns)\n",
    "\n",
    "    #Define most class id of interest and relabel\n",
    "    id_ofinterest = 203\n",
    "\n",
    "    labels_1vsall = multiclass_to_binary(labels, most_common_id=203)\n",
    "    \n",
    "    # #Set rng seed and permutation of data examples for training\n",
    "    # rng = check_random_state(5)\n",
    "\n",
    "    #Check number of dominant class examples\n",
    "    n_positive_class = np.sum(labels_1vsall)\n",
    "    \n",
    "    def feature_list_number_func(filename):\n",
    "        return int(filename.split('_')[0].split('/')[-1])\n",
    "\n",
    "    previous_features = glob.glob('features/*')\n",
    "    previous_features_sorted = sorted(previous_features,key=feature_list_number_func)\n",
    "\n",
    "    if len(previous_features_sorted) == 0: \n",
    "        feature_list_number = 1\n",
    "        pass\n",
    "    else:\n",
    "        previous_feature_data = pd.read_csv(previous_features_sorted[-1]) # read most recent feature reduction iteration\n",
    "        feature_list_number = feature_list_number_func(previous_features_sorted[-1]) + 1 # get current feature reduction iteration\n",
    "        previous_feature_keep = previous_feature_data.query(f\"Importances > {previous_feature_data['Importances'].quantile(q=0.25)}\") # get list of feature names with importances in the top 75%\n",
    "        previous_feature_names = previous_feature_keep['Name']\n",
    "\n",
    "        keep = np.isin(feature_names, previous_feature_names) # identify feature names that need to be kept\n",
    "\n",
    "        feature_names = feature_names[keep]\n",
    "        \n",
    "    ### Train, and get feature importances\n",
    "    subsampled_features, subsampled_targets = subsample_data(feat_df[feature_names].to_numpy(),labels_1vsall,n_positive_class)\n",
    "    prec, recall = run_random_forest(subsampled_features, subsampled_targets)\n",
    "    \n",
    "    ### Record\n",
    "    nfeats.append(len(feature_names))\n",
    "    precs.append(prec)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = pd.DataFrame({'n_features':nfeats, 'precision':precs, 'recall':recalls})\n",
    "stat_df.to_csv('pruning_stats.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_features</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1043</td>\n",
       "      <td>0.941123</td>\n",
       "      <td>0.975172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>782</td>\n",
       "      <td>0.959942</td>\n",
       "      <td>0.981708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>586</td>\n",
       "      <td>0.950145</td>\n",
       "      <td>0.980405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>439</td>\n",
       "      <td>0.953843</td>\n",
       "      <td>0.982361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>329</td>\n",
       "      <td>0.944195</td>\n",
       "      <td>0.981712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>246</td>\n",
       "      <td>0.945538</td>\n",
       "      <td>0.981712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>184</td>\n",
       "      <td>0.942125</td>\n",
       "      <td>0.979756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>138</td>\n",
       "      <td>0.947991</td>\n",
       "      <td>0.978440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103</td>\n",
       "      <td>0.948063</td>\n",
       "      <td>0.981708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>77</td>\n",
       "      <td>0.948434</td>\n",
       "      <td>0.982361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57</td>\n",
       "      <td>0.957851</td>\n",
       "      <td>0.986283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "      <td>0.946602</td>\n",
       "      <td>0.979102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31</td>\n",
       "      <td>0.944620</td>\n",
       "      <td>0.982370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23</td>\n",
       "      <td>0.947457</td>\n",
       "      <td>0.983673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0.948523</td>\n",
       "      <td>0.983015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>0.938506</td>\n",
       "      <td>0.978453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>0.936839</td>\n",
       "      <td>0.983669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>0.891595</td>\n",
       "      <td>0.966039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>0.894859</td>\n",
       "      <td>0.965385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0.892873</td>\n",
       "      <td>0.970609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0.898614</td>\n",
       "      <td>0.970609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.819936</td>\n",
       "      <td>0.919684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_features  precision    recall\n",
       "0         1043   0.941123  0.975172\n",
       "1          782   0.959942  0.981708\n",
       "2          586   0.950145  0.980405\n",
       "3          439   0.953843  0.982361\n",
       "4          329   0.944195  0.981712\n",
       "5          246   0.945538  0.981712\n",
       "6          184   0.942125  0.979756\n",
       "7          138   0.947991  0.978440\n",
       "8          103   0.948063  0.981708\n",
       "9           77   0.948434  0.982361\n",
       "10          57   0.957851  0.986283\n",
       "11          42   0.946602  0.979102\n",
       "12          31   0.944620  0.982370\n",
       "13          23   0.947457  0.983673\n",
       "14          17   0.948523  0.983015\n",
       "15          12   0.938506  0.978453\n",
       "16           9   0.936839  0.983669\n",
       "17           6   0.891595  0.966039\n",
       "18           4   0.894859  0.965385\n",
       "19           3   0.892873  0.970609\n",
       "20           2   0.898614  0.970609\n",
       "21           1   0.819936  0.919684"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f29cf555f1ad55622b61a06ba1522f7bf976ada2ce369c8ede82aeb1ba1ad311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
